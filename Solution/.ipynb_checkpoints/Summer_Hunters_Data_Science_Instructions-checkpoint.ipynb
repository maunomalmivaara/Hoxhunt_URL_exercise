{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HoxHunt Summer Hunters 2021 - Data - Home assignment\n",
    "\n",
    "\n",
    "<img src=\"http://hunters.hoxhunt.com/public/hero.svg\" width=\"800\">\n",
    "\n",
    "## Assignment\n",
    "\n",
    "In this assignment you as a HoxHunt Data Science Hunter are given the task to extract interesting features from a possible malicious indicator of compromise, more specifically in this case from a given potentially malicious URL. \n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/ao0neaphtfama7g/Screenshot%202019-03-21%2017.23.40.png?dl=1\" width=\"400\">\n",
    "\n",
    "This assignment assumes that you are comfortable (or quick to learn) on using Jupyter Notebooks and Python. You are free to use any external libraries you wish. We have included an example below using the Requests library.\n",
    "\n",
    "Happy hunting!\n",
    "\n",
    "\n",
    "## Interesting research papers & resources\n",
    "\n",
    "Below is a list of interesting research papers on the topic. They might give you good tips what features you could extract from a given URL:\n",
    "\n",
    "\n",
    "[Know Your Phish: Novel Techniques for Detecting\n",
    "Phishing Sites and their Targets](https://arxiv.org/pdf/1510.06501.pdf)\n",
    "\n",
    "[DeltaPhish: Detecting Phishing Webpages\n",
    "in Compromised Websites](https://arxiv.org/pdf/1707.00317.pdf)\n",
    "\n",
    "[PhishAri: Automatic Realtime Phishing Detection on Twitter](https://arxiv.org/pdf/1301.6899.pdf)\n",
    "\n",
    "[More or Less? Predict the Social Influence of Malicious URLs on Social Media\n",
    "](https://arxiv.org/abs/1812.02978)\n",
    "\n",
    "[awesome-threat-intelligence](https://github.com/hslatman/awesome-threat-intelligence)\n",
    "\n",
    "\n",
    "\n",
    "## What we expect\n",
    "\n",
    "Investigate potential features you could extract from a given URL, and implement extractors for the ones that interest you the most. The example code below extracts one feature, but does not store it very efficiently (just console logs it). Implement a sensible data structure using some known data structure library to store the features per URL. Choose one feature for which you can visualise the results. What does the visualisation tell you? Also consider how you would approach error handling, if one of the feature extractor fails?\n",
    "\n",
    "Should you make it to the next stage, be prepared to discuss the following topics: what features could indicate the malicousness of a given URL? What goes in to the thinking of the attacker when they are choosing a site for an attack? What inspired your solution and what would you develop next?\n",
    "\n",
    "## What we don't expect\n",
    "\n",
    "- That you implement a humangous set of features.\n",
    "- That you implement any kind of actual predicition models that uses the features to give predictions on malicousness at this stage.\n",
    "\n",
    "## Tips \n",
    "\n",
    "\n",
    "- Keep it tidy - a human is going to asses your work :)\n",
    "- Ensure your program does not contain any unwanted behaviour\n",
    "- What makes your solution stand out from the crowd?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My submission starts here:\n",
    "## #1: Preparation 1\n",
    "### The following cell includes:\n",
    "- Imports\n",
    "- Retrieval of Alexa's 1 million most visited sites\n",
    "- A naive (hand selected) set of non-malicious example URLs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>google.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>youtube.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>tmall.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>sohu.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>baidu.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492019</th>\n",
       "      <td>492020</td>\n",
       "      <td>translationwebservices.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492020</th>\n",
       "      <td>492021</td>\n",
       "      <td>tripleq.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492021</th>\n",
       "      <td>492022</td>\n",
       "      <td>ultimabg.com.au</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492022</th>\n",
       "      <td>492023</td>\n",
       "      <td>unicc.me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492023</th>\n",
       "      <td>492024</td>\n",
       "      <td>wsaquecedores.com.br</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>492024 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Rank                      Domain\n",
       "0            1                  google.com\n",
       "1            2                 youtube.com\n",
       "2            3                   tmall.com\n",
       "3            4                    sohu.com\n",
       "4            5                   baidu.com\n",
       "...        ...                         ...\n",
       "492019  492020  translationwebservices.com\n",
       "492020  492021                 tripleq.com\n",
       "492021  492022             ultimabg.com.au\n",
       "492022  492023                    unicc.me\n",
       "492023  492024        wsaquecedores.com.br\n",
       "\n",
       "[492024 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import requests\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Alexa's 1 million most visited sites:\n",
    "try: \n",
    "    alexa_top_million = pd.read_csv(\"http://s3.amazonaws.com/alexa-static/top-1m.csv.zip\", header=None)\n",
    "    alexa_top_million.columns = ['Rank', 'Domain']\n",
    "except IOError as e:\n",
    "    # If fails, \"throw\" error and initialize an empty DataFrame, so the code doesn't break:\n",
    "    print(e, \"- Alexa's top 1 million couldn't be retrieved\") # alternatively: raise e\n",
    "    alexa_top_million = pd.DataFrame(columns = ['Rank', 'Domain'])\n",
    "    \n",
    "\n",
    "# Naive set of examples for non-malicious urls (n=20):\n",
    "good_example_urls = [\"https://www.sanakirja.org/search.php?q=phishing&l=3&l2=17\",\n",
    "                     \"https://www.whatsapp.com/legal/terms-of-service-eea\",\n",
    "                     \"https://www.hs.fi/ulkomaat/art-2000007801776.html\",\n",
    "                     \"https://www.imdb.com/registration/signin?ref=nv_generic_lgin&u=%2F\",\n",
    "                     \"https://fi.hbonordic.com/my-account/subscription\",\n",
    "                     \"https://www.nytimes.com/2021/02/12/world/asia/china-world-health-organization-coronavirus.html\",\n",
    "                     \"https://www.quora.com/What-do-movies-always-get-wrong-about-your-profession\",\n",
    "                     \"https://stackoverflow.com/questions/1077347/hello-world-in-python\",\n",
    "                     \"https://www.wolframalpha.com/input/?i=x%5E2+%2B+4*x+-2+%2F+5a\",\n",
    "                     \"https://matlabacademy.mathworks.com/?s_tid=acb_tut\",\n",
    "                     \"https://www.rdocumentation.org/packages/visStatistics/versions/0.1.1\",\n",
    "                     \"https://www.terveyskirjasto.fi/terveyskirjasto/tk.koti?p_artikkeli=dlk01258&p_hakusana=korona\",\n",
    "                     \"https://autot.tori.fi/vaihtoautot/volvo/c30/78179405\",\n",
    "                     \"https://www.huuto.net/kohteet/poikien-bauer-vapor-x-15--merkkiset-luistimet/536832046\",\n",
    "                     \"https://www.microsoft.com/fi-fi/p/surface-laptop-go/94fc0bdgq7wv?icid=Cat-MSCOM-Hero1-P65657-CTA1\",\n",
    "                     \"https://www.kela.fi/toimeentulotuki-mihin-menoihin?inheritRedirect=true\",\n",
    "                     \"https://www.thesaurus.com/misspelling?term=url&s=t\",\n",
    "                     \"https://www.merriam-webster.com/words-at-play/the-words-of-the-week-february-12-2021\",\n",
    "                     \"https://usdgc.com/posts/youre-qualified-monday-qualifier-results/\",\n",
    "                     \"https://www.nhl.com/player/jesse-puljujarvi-8479344\"\n",
    "                    ]\n",
    "alexa_top_million"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #2: Preparation 2\n",
    "### The following cell includes:\n",
    "- The creation of a 200-element list of example phishing site urls, which is retrieved by crawling phishtank.com.\n",
    "    - This was done, because phishtank.com's API couldn't be used (because they didn't accept new registrations) and I couldn't find working alternatives by a quick googling. It's a bit of an overkill, but works fine. It's **slow** though..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bad_urls(nof_pages):\n",
    "    # This function returns a list containing nof_pages * 20 phishing urls.\n",
    "    \n",
    "    def get_urls_of_next_page(from_url, iteration):\n",
    "        # This function is called recursively. It goes through the page found from 'from_url',\n",
    "        # which is expected to contain a table of known phishing urls. Some of the url strings are cut short\n",
    "        # for space, so the link to the 'detail page' of each url is visited, and the entire url is\n",
    "        # retrieved from there.\n",
    "        \n",
    "        # 1) Find all hrefs from from_page html:\n",
    "        \n",
    "        content = requests.get(from_url).content\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        \n",
    "        hrefs = list()\n",
    "        for link in soup.find_all('a'):\n",
    "            hrefs.append(link.get('href'))\n",
    "        \n",
    "        # 2) Filter the hrefs to include only the phish 'detail pages':\n",
    "        \n",
    "        def is_phish_detail_link(e):\n",
    "            return e.startswith('phish_detail.php')\n",
    "        hrefs = list(filter(is_phish_detail_link, hrefs))\n",
    "        \n",
    "        # 3) Get the html content of each detail page, and extract the entire phishing url:\n",
    "        \n",
    "        bad_urls = list()\n",
    "        for href in hrefs:\n",
    "            url = 'https://www.phishtank.com/' + href\n",
    "            c = requests.get(url).content\n",
    "            s = BeautifulSoup(c, 'html.parser')\n",
    "            \n",
    "            # The phishing url is extracted here:\n",
    "            url_element = s.find_all('b')[1]\n",
    "            \n",
    "            # If the url contains an email address (very few seem to), the email should be decrypted to make\n",
    "            # BeautifulSoap understand it, but for the sake of this exercise it is just replaced with an\n",
    "            # example string.\n",
    "            # (A random number between 0 and 1000 is added to prevent duplicates later.)\n",
    "            if (url_element.find('a')):\n",
    "                example_string = f\"example.html?email={random.randrange(1000)}@b.com\"\n",
    "                phish_url = url_element.contents[0] + example_string\n",
    "            else:\n",
    "                phish_url = url_element.decode_contents()\n",
    "                \n",
    "            # Add to the list of urls:\n",
    "            bad_urls.append(phish_url)\n",
    "            \n",
    "\n",
    "        # 4) If the specified number of pages hasn't yet been gone through, recursively\n",
    "        # call this function again for the site containing older phishing urls:\n",
    "        \n",
    "        if (iteration < nof_pages):\n",
    "            # Get link to older phishing urls:\n",
    "            link_to_older_page = soup.find('a', text=re.compile('Older'))['href']\n",
    "            url_of_older_page = 'https://www.phishtank.com/phish_search.php' + link_to_older_page\n",
    "            # Call this function again:\n",
    "            bad_urls = bad_urls + get_urls_of_next_page(url_of_older_page, iteration + 1)\n",
    "        \n",
    "        return bad_urls\n",
    "\n",
    "    start_url = 'https://www.phishtank.com/phish_search.php?valid=y&active=All&Search=Search'\n",
    "    return get_urls_of_next_page(start_url, 1)\n",
    "\n",
    "\n",
    "# Get 200 (=10*20) verified phishing site urls:\n",
    "bad_example_urls = get_bad_urls(10)\n",
    "\n",
    "pd.DataFrame(bad_example_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #3: Analysis\n",
    "### The following cell includes:\n",
    "- Analysis of the lists of example URLs.\n",
    "    - 9 features are extracted from the URLs, as according to Marchal et al. (2016)\n",
    "- The creation of corresponding DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_domain_from_url(url):\n",
    "    # This function returns the domain of a url.\n",
    "    t = urlparse(url).netloc\n",
    "    return '.'.join(t.split('.')[-2:])\n",
    "\n",
    "def parse_hostname_from_url(url):\n",
    "    # This function returns the hostname of a url.\n",
    "    return urlparse(url).hostname\n",
    "\n",
    "def parse_freeURL_from_url(url):\n",
    "    # This function returns the freeURL of a url. That is, the subdomains, the path, and queries.\n",
    "    subdomains = parse_subdomains_from_url(url)\n",
    "    path_and_queries = parse_path_and_query_from_url(url)\n",
    "    freeURL = subdomains + path_and_queries\n",
    "    return freeURL\n",
    "\n",
    "def parse_subdomains_from_url(url):\n",
    "    # This function returns the subdomains of a url.\n",
    "    # (Subdomains = hostname - domain)\n",
    "    domain = parse_domain_from_url(url)\n",
    "    hostname = parse_hostname_from_url(url)\n",
    "    index_of_domain = hostname.find(domain)\n",
    "    return hostname[0:index_of_domain]\n",
    "\n",
    "def find_nth(string, substring, n):\n",
    "    # This function returns the index of the nth occurrance of substring in string.\n",
    "    if (n == 1):\n",
    "        return string.find(substring)\n",
    "    else:\n",
    "        return string.find(substring, find_nth(string, substring, n - 1) + 1)\n",
    "    \n",
    "def parse_path_and_query_from_url(url):\n",
    "    # This function returns the path and queries of a url.\n",
    "    # (The path and queries start after the 3rd slash of a url)\n",
    "    i = find_nth(url, '/', 3)\n",
    "    return url[i:]\n",
    "\n",
    "def parse_mld_from_domain(domain):\n",
    "    # This function returns the mld of a domain.\n",
    "    return domain.split('.')[0]\n",
    "    \n",
    "def get_nof_dots(string):\n",
    "    # This function returns the number of dots in a string.\n",
    "    return string.count('.')\n",
    "\n",
    "def uses_https(url):\n",
    "    # This function returns a boolean which indicates whether a url uses https or not.\n",
    "    return urlparse(url).scheme == \"https\"\n",
    "\n",
    "def get_nof_level_domains(hostname):\n",
    "    # This function returns the number of level domains in a hostname.\n",
    "    domains = hostname.split('.')\n",
    "    return len(domains)\n",
    "\n",
    "def get_nof_terms_in_url(url):\n",
    "    # This function returns the number of terms in a url.\n",
    "    delimiters = [\".\", \"/\", \":\", \"-\"]\n",
    "    regexPattern = '|'.join(map(re.escape, delimiters))\n",
    "    # Get list of terms in url:\n",
    "    s = list(filter(None, re.split(regexPattern, url)))\n",
    "    return len(s)\n",
    "\n",
    "def get_nof_terms_in_mld(mld):\n",
    "    # This function returns the number of terms in an mld.\n",
    "    # (Assumes that mlds can only be split by a dash)\n",
    "    return len(mld.split('-'))\n",
    "\n",
    "def get_alexa_rank(domain):\n",
    "    # This function returns the Alexa rank of a domain.\n",
    "    rank = list(alexa_top_million[alexa_top_million['Domain'] == domain]['Rank'])\n",
    "    if (len(rank) > 0):\n",
    "        return rank[0]\n",
    "    return None\n",
    "\n",
    "def analyze_url(url):\n",
    "    \n",
    "    # This function extracts a set of features for a given URL.\n",
    "    # The URL features are extracted according to the framework by Marchal et al. (2016).\n",
    "\n",
    "    domain = parse_domain_from_url(url)\n",
    "    freeURL = parse_freeURL_from_url(url)\n",
    "    hostname = parse_hostname_from_url(url)\n",
    "    mld = parse_mld_from_domain(domain)\n",
    "\n",
    "    data = {}\n",
    "    # 1 Protocol used:\n",
    "    data['uses_https'] = uses_https(url)\n",
    "    # 2 Number of dots in freeURL:\n",
    "    data['nof_dots_in_freeURL'] = get_nof_dots(freeURL)\n",
    "    # 3 Number of level domains:\n",
    "    data['nof_level_domains'] = get_nof_level_domains(hostname)\n",
    "    # 4 Length of URL:\n",
    "    data['url_length'] = len(url)\n",
    "    # 5 Length of FQDN (hostname):\n",
    "    data['fqdn_length'] = len(hostname)\n",
    "    # 6 Length of mld:\n",
    "    data['mld_length'] = len(mld)\n",
    "    # 7 Number of terms in url:\n",
    "    data['nof_terms_in_url'] = get_nof_terms_in_url(url)\n",
    "    # 8 Number of terms in mld:\n",
    "    data['nof_terms_in_mld'] = get_nof_terms_in_mld(mld)\n",
    "    # 9 Alexa ranking of url:\n",
    "    data['alexa_rank'] = get_alexa_rank(domain)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_url_dataframe(url_list):\n",
    "    # This function takes in a list of urls and returns a dataframe containing the url features.\n",
    "    url_data = dict()\n",
    "    for url in url_list:\n",
    "        url_data[url] = analyze_url(url)\n",
    "        \n",
    "    return pd.DataFrame(url_data)\n",
    "\n",
    "# The resulting dataframes:\n",
    "url_data_bad = get_url_dataframe(bad_example_urls)\n",
    "url_data_good = get_url_dataframe(good_example_urls)\n",
    "\n",
    "url_data_bad.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #4: Visualization\n",
    "### The following cell includes:\n",
    "- A visualization of the main level domain (mld) lengths of the example urls.\n",
    "    - The resulting graph represents the empirical probabilities of the mld lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "\n",
    "def plot_hist(df, description, color, edgecolor):\n",
    "    # This function returns a pyplot histogram of the mld lengths of the given url dataframe.\n",
    "    \n",
    "    wanted_data = df.iloc[5]\n",
    "    w = 2\n",
    "    # Number of bins according to bin width (w):\n",
    "    bins = math.ceil((wanted_data.max() - wanted_data.min())/w)\n",
    "    # Return histogram:\n",
    "    return plt.hist(wanted_data,\n",
    "                    bins,\n",
    "                    color=color,\n",
    "                    alpha=0.5,\n",
    "                    label=description + f' (n={len(wanted_data)})',\n",
    "                    edgecolor=edgecolor,\n",
    "                    linewidth=1.2,\n",
    "                    density=1)\n",
    "\n",
    "# Plot both bad and good urls:\n",
    "plot_hist(url_data_bad, 'Phishing URLs', 'r', 'red')\n",
    "plot_hist(url_data_good, 'Safe URLs', 'g', 'green')\n",
    "\n",
    "plt.title(f'Probability Distribution of MLD Lengths')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Empirical Probability')\n",
    "plt.xlabel('MLD Length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #5: Interpretation of Visualization\n",
    "\n",
    "### 5.1: Findings:\n",
    "- As can be seen from the above graph, the MLD lengths of \"safe\" URLs all seem to be around 2-15.\n",
    "- By contrast, the MLDs of phishing URLs tend to be much longer.\n",
    "- The distribution of the phishing URLs has a lower kurtosis than the safe URLs, i.e. the variance of the phishing URL MLD lengths is much larger the one of the safe URLs. As expected, the distribution of the MLD lengths of phishing URLs has a long right tail.\n",
    "\n",
    "### 5.2: Conclusions:\n",
    "- The MLD lengths of safe URLs tend to be under 20 characters.\n",
    "- The MLD lengths of phishing URLs can range far beyond 20 characters.\n",
    "- Therefore, **a URL with a long MLD can be a signal of a malicious phishing site.**\n",
    "\n",
    "\n",
    "## #6: Limitations\n",
    "\n",
    "- The sample size for the safe URLs was very small (n=20) and they were hand-picked. Therefore, the sample cannot be treated as a true representation of the whole population of safe URLs.\n",
    "- It seems that phishing sites reported to phishtank.com tend to be submitted in chunks, which sometimes contain multiple nearly identical URLs. This can skew the results towards certain types of URLs.\n",
    "\n",
    "## #7 Error Handling\n",
    "\n",
    "- The only feature which takes use of external information is Alexa ranking. If, for some reason, the top 1 million Alexa Rankings couldn't be fetched, the dataframe containing the Alexa Rankings ('alexa_top_million') is returned **empty** and an error is raised.\n",
    "- If errors occur in the extracting of other features, it is due to bugs in code, which have to be assessed manually."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
